{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UC_Lab 7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPkJ4K5ZK9AQl5SGiWHfd+H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmadIsmail83N/MyData/blob/master/UC_Lab_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NBP1o8EPOte",
        "colab_type": "text"
      },
      "source": [
        "**Ensure using TensorFlow 1.x**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dn411C4UMuVk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "66ea6d56-ba90-4e61-ff23-ac8eeb42cd5c"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toziskl0PVbQ",
        "colab_type": "text"
      },
      "source": [
        "**Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvaMbIrzHzSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "from sklearn.datasets import fetch_20newsgroups"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3IvBrbdPeF4",
        "colab_type": "text"
      },
      "source": [
        "**Get Datasets according to variant 14**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8PvjilkIO43",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "e1dd7df6-2002-42e4-f595-c0ffa5712d04"
      },
      "source": [
        "categories = [\"comp.sys.ibm.pc.hardware\",\"rec.motorcycles\",\"sci.electronics\",\"alt.atheism\"]\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
        "print('total texts in train:',len(newsgroups_train.data))\n",
        "print('total texts in test:',len(newsgroups_test.data))\n",
        "print('text',newsgroups_train.data[0])\n",
        "print('category:',newsgroups_train.target[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total texts in train: 2259\n",
            "total texts in test: 1502\n",
            "text From: bil@okcforum.osrhe.edu (Bill Conner)\n",
            "Subject: Re: Americans and Evolution\n",
            "Nntp-Posting-Host: okcforum.osrhe.edu\n",
            "Organization: Okcforum Unix Users Group\n",
            "X-Newsreader: TIN [version 1.1 PL9]\n",
            "Lines: 30\n",
            "\n",
            "Robert Singleton (bobs@thnext.mit.edu) wrote:\n",
            "\n",
            ": > Sure it isn't mutually exclusive, but it lends weight to (i.e. increases\n",
            ": > notional running estimates of the posterior probability of) the \n",
            ": > atheist's pitch in the partition, and thus necessarily reduces the same \n",
            ": > quantity in the theist's pitch. This is because the `divine component' \n",
            ": > falls prey to Ockham's Razor, the phenomenon being satisfactorily \n",
            ":                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            ": > explained without it, and there being no independent evidence of any \n",
            ":   ^^^^^^^^^^^^^^^^^^^^\n",
            ": > such component. More detail in the next post.\n",
            ": > \n",
            "\n",
            "Occam's Razor is not a law of nature, it is way of analyzing an\n",
            "argument, even so, it interesting how often it's cited here and to\n",
            "what end. \n",
            "It seems odd that religion is simultaneously condemned as being\n",
            "primitive, simple-minded and unscientific, anti-intellectual and\n",
            "childish, and yet again condemned as being too complex (Occam's\n",
            "razor), the scientific explanation of things being much more\n",
            "straightforeward and, apparently, simpler. Which is it to be - which\n",
            "is the \"non-essential\", and how do you know?\n",
            "Considering that even scientists don't fully comprehend science due to\n",
            "its complexity and diversity. Maybe William of Occam has performed a\n",
            "lobotomy, kept the frontal lobe and thrown everything else away ...\n",
            "\n",
            "This is all very confusing, I'm sure one of you will straighten me out\n",
            "tough.\n",
            "\n",
            "Bill\n",
            "\n",
            "category: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmGHJj3-Pl2X",
        "colab_type": "text"
      },
      "source": [
        "**Generate Array of words from text of each collection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQGMU_GaHwOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Counter()\n",
        "for text in newsgroups_train.data:\n",
        "  for word in text.split(' '):\n",
        "      vocab[word.lower()] += 1\n",
        "\n",
        "for text in newsgroups_test.data:\n",
        "  for word in text.split(' '):\n",
        "      vocab[word.lower()] += 1\n",
        "\n",
        "total_words = len(vocab)\n",
        "\n",
        "def get_word_2_index(vocab):\n",
        "  word2index = {}\n",
        "  for i, word in enumerate(vocab):\n",
        "    word2index[word.lower()] = i\n",
        "  return word2index\n",
        "\n",
        "word2index = get_word_2_index(vocab)\n",
        "\n",
        "def get_batch(df, i, batch_size):\n",
        "  batches = []\n",
        "  results = []\n",
        "  texts = df.data[i * batch_size:i * batch_size + batch_size]\n",
        "  categories = df.target[i * batch_size:i * batch_size + batch_size]\n",
        "  for text in texts:\n",
        "    layer = np.zeros(total_words, dtype=float)\n",
        "    for word in text.split(' '):\n",
        "      layer[word2index[word.lower()]] += 1\n",
        "    batches.append(layer)\n",
        "\n",
        "  for category in categories:\n",
        "    y = np.zeros((3), dtype=float)\n",
        "    if category == 0:\n",
        "      y[0] = 1.\n",
        "    elif category == 1:\n",
        "      y[1] = 1.\n",
        "    else:\n",
        "      y[2] = 1.\n",
        "    results.append(y)\n",
        "\n",
        "  return np.array(batches), np.array(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41cpbtrsQO_B",
        "colab_type": "text"
      },
      "source": [
        "**Neural NetWork: Parameters and design : 3 hidden layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S_f3DEyMHZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Параметры обучения\n",
        "learning_rate = 0.07\n",
        "training_epochs = 10\n",
        "batch_size = 150\n",
        "display_step = 1\n",
        "# Network Parameters\n",
        "n_hidden_1 = 35 # скрытый слой\n",
        "n_hidden_2 = 25 # скрытый слой\n",
        "n_hidden_3 = 20 # скрытый слой\n",
        "n_input = total_words # количество уникальных слов в наших текстах\n",
        "n_classes = 3 # 3 класса\n",
        "input_tensor = tf.placeholder(tf.float32,[None, n_input],name=\"input\")\n",
        "output_tensor = tf.placeholder(tf.float32,[None, n_classes],name=\"output\")\n",
        "def multilayer_perceptron(input_tensor, weights, biases):\n",
        "  # скрытый слой\n",
        "  layer_1_multiplication = tf.matmul(input_tensor, weights['h1'])\n",
        "  layer_1_addition = tf.add(layer_1_multiplication, biases['b1'])\n",
        "  layer_1 = tf.nn.relu(layer_1_addition)\n",
        "  # скрытый слой\n",
        "  layer_2_multiplication = tf.matmul(layer_1, weights['h2'])\n",
        "  layer_2_addition = tf.add(layer_2_multiplication, biases['b2'])\n",
        "  layer_2 = tf.nn.relu(layer_2_addition)\n",
        "  # скрытый слой\n",
        "  layer_3_multiplication = tf.matmul(layer_2, weights['h3'])\n",
        "  layer_3_addition = tf.add(layer_3_multiplication, biases['b3'])\n",
        "  layer_3 = tf.nn.relu(layer_3_addition)\n",
        "  # выходной слой\n",
        "  out_layer_multiplication = tf.matmul(layer_3, weights['out'])\n",
        "  out_layer_addition = out_layer_multiplication + biases['out']\n",
        "  return out_layer_addition"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i47q02kNMhn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# инициализация параметров сети\n",
        "weights = {\n",
        "'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
        "'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
        "'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
        "'out': tf.Variable(tf.random_normal([n_hidden_3, n_classes]))\n",
        "}\n",
        "biases = {\n",
        "'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
        "'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
        "'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
        "'out': tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        "# создание модели\n",
        "prediction = multilayer_perceptron(input_tensor, weights, biases)\n",
        "# Фукнция потерь\n",
        "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=output_tensor))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
        "init = tf.global_variables_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8rQ19KEM95x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "4307a65f-15c7-4ca4-9ee3-acfdc582b6e1"
      },
      "source": [
        "# Запуск\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  # Цикл обучения\n",
        "  for epoch in range(training_epochs):\n",
        "    avg_cost = 0.\n",
        "    total_batch = int(len(newsgroups_train.data)/batch_size)\n",
        "    # Проход по всем батчам\n",
        "    for i in range(total_batch):\n",
        "      batch_x,batch_y = get_batch(newsgroups_train,i,batch_size)\n",
        "      c,_ = sess.run([loss,optimizer], feed_dict={input_tensor: batch_x,output_tensor:batch_y})\n",
        "      # Вычисляем среднее фукнции потерь\n",
        "      avg_cost += c / total_batch\n",
        "    print(\"Эпоха:\", '%04d' % (epoch+1), \"loss=\", \"{:.16f}\".format(avg_cost))\n",
        "  print(\"Обучение завершено!\")\n",
        "\n",
        "  # Тестирование\n",
        "  correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(output_tensor, 1))\n",
        "  # Расчет точности\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  total_test_data = len(newsgroups_test.target)\n",
        "  batch_x_test,batch_y_test = get_batch(newsgroups_test,0,total_test_data)\n",
        "  print(\"Точность:\", accuracy.eval({input_tensor: batch_x_test, output_tensor: batch_y_test}))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Эпоха: 0001 loss= 370.3777043660481354\n",
            "Эпоха: 0002 loss= 37.0278567314147864\n",
            "Эпоха: 0003 loss= 10.0832759857177727\n",
            "Эпоха: 0004 loss= 9.8718433698018391\n",
            "Эпоха: 0005 loss= 18.3314972122510227\n",
            "Эпоха: 0006 loss= 7.9368372639020279\n",
            "Эпоха: 0007 loss= 7.2169995983441666\n",
            "Эпоха: 0008 loss= 3.8342475454012548\n",
            "Эпоха: 0009 loss= 1.0332314416766166\n",
            "Эпоха: 0010 loss= 0.4355002654095490\n",
            "Обучение завершено!\n",
            "Точность: 0.75233024\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}